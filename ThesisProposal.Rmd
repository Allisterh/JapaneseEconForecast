### Title  
##### Forecasting Japanese Macroeconomy Using High-Dimensional Data
  
### Background  
* マクロデータの特徴として計測頻度が少ない(月次, 四半期)一方で, 計測している系列の種類は比較的多い.   
* このことは, 従来の計量経済学の手法(VARナド)を行う上での推定を困難にしている. (degrees of freedom problem, curse of dimentionality, over-parametarisation)  
* そのため, 従来の手法ではeconometrician/practitioner が少数の変数を選んでモデルを構築していた. しかし, 実際の経済では多くの変数が互いに影響を与えていることや, 政策決定者はモデルに含まれている変数以上の数の指標を見ている → 金融政策決定におけるモデルと, parsimoniousな統計モデルの乖離 (Bernanke et al, 2005)
* この問題を克服するための手法が近年, 計量経済学/機械学習分野で発展. 代表的なものとしてLASSOとFactor model (及びその派生系)  
* これらの手法を用いることで, 実際に予測の精度が向上することが確認されている(Stock and Watson, 2002, Ludvigson and Ng, 2009, Li and Chen 2014, Callot and Kock, 2014, etc.)  
* マクロ変数の予測はマクロ経済学のみならず, ファイナンスや金融政策などにおいても重要な役割を果たしている (Li and Chen, 2014)
	* Bernanke et al (2005): FAVARでImpulse responseがこれまで以上の変数に対して得られることによるマクロ変数の経済の動学構造の理解(予測というよりは金融政策の影響の分析)
	* Rapach, Strauss and Zhou (2010): 自国および他国のエクイティの影響を持つUSのマクロ経済ファンダメンタルズの予測のポートフォリオマネジメントにおける重要性
	* Ludvingson and Ng (2009): マクロ変数を用いることで債権市場の超過収益が予測可能
	* Welfe (2013): 正確なモデルを用いた予測やシミュレーションは, 経済全般の正しい理解とそれに基づく政策決定に貢献?
	* Bai and Ng (2008): マクロ変数の中でも特にインフレ率は, 民間および政府機関の意思決定に重要でありながら, 予測が困難な変数として知られる.   
* 日本での高次元データを利用したマクロ実証研究としては, Shibamoto(2007)のFAVARを用いた金融政策ショックの分析, 飯星(2009)による金融政策の評価, 早川・小林(2011)の景気動向指数の作成(Nowcasting)など. 日本経済のマクロ予測を行った研究はおそらくない(Shintani, 2005があった)  
  
### Literature Review  
* Stock and Watson (1999)... Phillips Curveを用いたインフレ率予測に168の米マクロデータから取り出した共通因子を合わせて用いることでインフレ予測の精度の向上.   
* Zou, Hastie and Tibshirani (2006) ... PCA のfactor loadings に対してsparse性を課した (LASSOの次元削減のアイディアをfactor loadingsに応用した)SPCAを提案.  遺伝子データとシミュレーションデータから, SPCAは予測の精度を下げることなく解釈を容易にしたことを示す  
* Callot and Kock (2014)... VARの枠組みにL1正則化を適用することで変数およびラグ数をモデルが選択. アメリカの主要なマクロ変数に対する予測をLasso, agaptive lasso, group lasso and adptive group lassoで行う. Lassoでの予測がもっともMSFEが小さい. またaLasso, agLasso\*がoracle property を持つことを証明(\*groupが正しく分類されている場合)  
* Li and Chen (2014)... Lasso, elastic net, group lassoを用いて米の主要マクロ変数を予測. これらの手法はDFMに比べてMSFEが小さく, ENET, gLassoは変数選択が予測期間を通じて安定しており, interpretabilityも高いことを示す  
* Nicholsen, Matteson and Bien (2017)... Lasso VARに外生変数を加えたVARXを用いることで, 石油価格などの外生的なショック組み込む. 米マクロデータとシミュレーションデータを用いて予測の精度の向上\*. `BigVAR` packageを作りCRANで公開. (他の研究に比べてマクロデータでの改善は小さい. ファクターモデルはsample meanとほぼ変わらず)

### Methodology  
* lasso family: **lasso**, **Elastic Net**, Adaptive lasso, group lasso, adaptive group lasso + some from NMB(2017) paper and `BigVAR` package  
* Factor model: **dynamic facotor model** , SPCA  
(太字が現時点で結果が得られているもの, ENETに関してはh=1,3ではまだできておらず, CVにも不備があるため結果は現時点ではあまり信用できない)  

* lasso, elastic netでは, Cross validationによりtuning parameterとラグ数を選ぶ(詳しくはResultsのセクションで)  

* Factor modelは２段階で行う. １段階目で主成分分析によってファクターを推定. 推定されたファクターおよび, 予測したい変数のラグを用いたDI予測を２段階目で行う. 予測したい変数のラグ数はBICで選択  
Mean Squared Forecast Errorによって予測の精度を評価する  
$h > 1$ の際の予測はdirect forecasting.  
構造変化の可能性が考えられるため, rolling windowを用いる  

比較のためのモデルとして, Sample mean (観測期間の平均を予測値とする), Random Walk (t期の観測値がt+h期の予測値), AR (BICでラグ選択), 3または4変数VAR(BICでラグ選択)を行った. 



### Implementation  
* lasso: `glmnet`, `grplasso`, `BigVAR` and `lars`
* factor: `base` 

### Dataset
SW, 早川・小林(2011)を参考に128の系列を様々なソースから収集した(経産省, 日銀, データストリームなど)  
期間は2003年1月~2018年6月(2003年4月に変更予定[マネーストックについて統計手法の変更がある]).  
基本的には, 株価・金利・為替をのぞいて季節調整. 季調前の系列しか収集できない場合は`x12` packageを用いて変換(X13-ARIMA-SEATS)   
単位根検定(ADFおよびPP)を行った上で, 失業率・求人倍率・金利などは差分, その他の系列については対数の差分をとった(\*CPIの季調を忘れていた)
それでも単位根が残ったものに関してはさらにもう一度差分をとった(\*ADFまたはPPのどちらかで単位根があった場合には単位根が残っているとした, 解釈が不明になるので１階差分でやめておくべき？[１回差分の時点で単位根はほぼ解消されていた])  
変換の方法はSW, 早川・小林(2011)を参考  

### Results  
変換後に利用可能な観測数184を３つに分ける.  
* 2003/03 ~ 2008/06 (Initialisation) : LASSOのみで使用. Tuning parameter (regularisation parameterとlassoとridgeのweight) およびラグ数を変動させながら, Parameter selection period の予測を行い, もっともMSFEの小さいパラメータを選択する (Cross validation?). 観測数はpやhによって変化する(最初のp個の観測分は2003/02以前の観測値を用いなければならずNA)    
* 2008/07 ~ 2013/06 (Parameter Selection):  Initialisation periodが予測する期間(\# of obs = 60). また, この期間の観測を用いてForecast evaluation periodの予測を行う. (景気の停滞期を重なっているのが心配)  
* 2013/07~2018/06 (Forecast Evaluation): 最終的なMSFEに用いられる予測が作成される期間.  


##### Factor models  
* Bai & Ng (2002) の情報量基準を用いてファクター数を決定したところ, ファクター数(r)が16~19とかなり多くなる. これはファクターの動学的な構造を考慮していないことによると考えられたが, Amengual & Watson (2007) による動学的ファクター数(q)の情報量基準でも13~18のファクター数が推定された --> 早川・小林(2011)ではr=20となっていたので日本のデータではrは多くなりがち？(ただしAltissimo et al (2001)によるqは4).  
* 予測の精度は全ての変数, forecasting horizonにおいてARを上回る  

##### LASSO, Elastic Net  
* あまり予測の精度は高いとは言えない(benchmarkを上回ったり下回ったり)  
* horizonが長い方が比較的精度が高い. 
* sparsity ratio はラグの長さによって分母が変わってくるので一概には言えない(選択されたラグ数を記録していなかった). sparsity ratio が0のモデルも散見される. 

### 課題  
* 解釈が明瞭でない. (ファクター数が多い, モデルによって選ばれる変数が異なる) --> ファクター数を少なくする, データセットをファクターに回帰して, 変数ごと(or カテゴリーごとのファクターによる説明度合を調べる. group lasso, elastic netなど解釈しやすいモデルを用いる  
* 別の基準でrを選択する？--> ファクターで説明できるデータの分散(R^2のようなもの)について何らかの閾値を設定する  
* sparsity ratio が低い --> cross validationを行なっている期間(2008/07~2013/06)が特殊なため, lambdaの値が大きくなる？
* そもそもデータの変換はこれで良いのか(second differenceの解釈,first difference でもADFなら単位根は消えている)  
* 計算にかなり時間がかかる--> ラグ選択でCVを避ける(Wang et al (2007)の高次元BICを利用する), 予測する変数を絞る  
* 点推定(予測)に加えて、分布や分位点について知ることができれば、portfolio managementにおけるリスク管理などについても応用できる可能性が高まる(Rossi et al, ?)





